{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "## Importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "7af7cef5d3d3ef24b4df12fa13b429aaf93a0c1e"
   },
   "outputs": [],
   "source": [
    "## Step 1: Importing data from source\n",
    "dataset = pd.read_csv(\"/Users/malarvizhis/Desktop/Pokemon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "65947793ca800479b0743ef54a1dbb1f4f6abe1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   #                   Name Type 1  Type 2  Total  HP  Attack  Defense  \\\n",
      "0  1              Bulbasaur  Grass  Poison    318  45      49       49   \n",
      "1  2                Ivysaur  Grass  Poison    405  60      62       63   \n",
      "2  3               Venusaur  Grass  Poison    525  80      82       83   \n",
      "3  3  VenusaurMega Venusaur  Grass  Poison    625  80     100      123   \n",
      "4  4             Charmander   Fire     NaN    309  39      52       43   \n",
      "\n",
      "   Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
      "0       65       65     45           1      False  \n",
      "1       80       80     60           1      False  \n",
      "2      100      100     80           1      False  \n",
      "3      122      120     80           1      False  \n",
      "4       60       50     65           1      False  \n",
      "(800, 13)\n",
      "RangeIndex(start=0, stop=800, step=1)\n",
      "Index([u'#', u'Name', u'Type 1', u'Type 2', u'Total', u'HP', u'Attack',\n",
      "       u'Defense', u'Sp. Atk', u'Sp. Def', u'Speed', u'Generation',\n",
      "       u'Legendary'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Analyzing the structure and aspects of data\n",
    "print(dataset.head(5))\n",
    "print(dataset.shape)\n",
    "print(dataset.index)\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "2c420c6522ce0078ebcc950a4fd225cf8877b1ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#             0\n",
      "Name          0\n",
      "Type 1        0\n",
      "Type 2        0\n",
      "Total         0\n",
      "HP            0\n",
      "Attack        0\n",
      "Defense       0\n",
      "Sp. Atk       0\n",
      "Sp. Def       0\n",
      "Speed         0\n",
      "Generation    0\n",
      "Legendary     0\n",
      "dtype: int64\n",
      "(414, 13)\n"
     ]
    }
   ],
   "source": [
    "## Processing the Data\n",
    "## Handling of missing values\n",
    "## Here are two techniques to handle missing values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Technique 1: we either delete a particular row if it has a null value for a particular feature and a particular column if it has more than 75% of missing values.This method is advisable only we have enough dataset\n",
    "## Note: Deletion of Data will lead to loss of information which might affect the prediction results.\n",
    "\n",
    "\n",
    "dataset.dropna(inplace=True)\n",
    "print(dataset.isnull().sum())\n",
    "print(dataset.shape)\n",
    "\n",
    "## result you can see there is reduction in the rows nearly half of the source dataset. - if you are running cell it may affect below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "8f2e3760e741073010ab2550f81e3d2734e33597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       45\n",
      "1       60\n",
      "2       80\n",
      "3       80\n",
      "4       65\n",
      "5       80\n",
      "6      100\n",
      "7      100\n",
      "8      100\n",
      "9       43\n",
      "10      58\n",
      "11      78\n",
      "12      78\n",
      "13      45\n",
      "14      30\n",
      "15      70\n",
      "16      50\n",
      "17      35\n",
      "18      75\n",
      "19     145\n",
      "20      56\n",
      "21      71\n",
      "22     101\n",
      "23     121\n",
      "24      72\n",
      "25      97\n",
      "26      70\n",
      "27     100\n",
      "28      55\n",
      "29      80\n",
      "      ... \n",
      "770     60\n",
      "771    118\n",
      "772    101\n",
      "773     50\n",
      "774     40\n",
      "775     60\n",
      "776     80\n",
      "777     75\n",
      "778     38\n",
      "779     56\n",
      "780     51\n",
      "781     56\n",
      "782     46\n",
      "783     41\n",
      "784     84\n",
      "785     99\n",
      "786     69\n",
      "787     54\n",
      "788     28\n",
      "789     28\n",
      "790     55\n",
      "791    123\n",
      "792     99\n",
      "793     99\n",
      "794     95\n",
      "795     50\n",
      "796    110\n",
      "797     70\n",
      "798     80\n",
      "799     70\n",
      "Name: Speed, Length: 800, dtype: int64\n",
      "#             False\n",
      "Name          False\n",
      "Type 1        False\n",
      "Type 2         True\n",
      "Total         False\n",
      "HP            False\n",
      "Attack        False\n",
      "Defense       False\n",
      "Sp. Atk       False\n",
      "Sp. Def       False\n",
      "Speed         False\n",
      "Generation    False\n",
      "Legendary     False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "## Since the dataset is quite minimal, Let's try the alternative method for handling missing values\n",
    "\n",
    "## Technique 2 - This can be applied on features which has numerical data like year, values etc. This is an approximation which adds variance to the dataset but can avoid loss of data\n",
    "## It's a standard technique and for this dataset we have mix of both numerical and categorical data.\n",
    "\n",
    "## Numerical NaN \n",
    "## In the given dataset below are the features with numerical values\n",
    "\n",
    "## Features :[Total,HP,Attack,Defense,Sp.Atk,Sp.Def,Speed]\n",
    "## Note: Generation is a numerical value but those values are categorical so we are not considering it.\n",
    "\n",
    "# print(dataset['Total'].mean())\n",
    "# print(dataset['Total'].tail())\n",
    "\n",
    "dataset['Total']= dataset['Total'].fillna(dataset['Total'].mean())\n",
    "\n",
    "\n",
    "\n",
    "## Similar technique to be adopted for other numerical columns\n",
    "\n",
    "# print(dataset['HP'].mean())\n",
    "# print(dataset['HP'].tail())\n",
    "\n",
    "dataset['HP']= dataset['HP'].replace(np.NaN,dataset['HP'].mean())\n",
    "\n",
    "# print(dataset['Attack'].mean())\n",
    "# print(dataset['Attack'].tail())\n",
    "\n",
    "dataset['Attack'] = dataset['Attack'].replace(np.NaN,dataset['Attack'].mean())\n",
    "# print(dataset['Defense'].mean())\n",
    "# print(dataset['Defense'].tail())\n",
    "\n",
    "dataset['Defense'] = dataset['Defense'].replace(np.NaN,dataset['Defense'].mean())\n",
    "# print(dataset['Sp. Atk'].mean())\n",
    "# print(dataset['Sp. Atk'].tail())\n",
    "\n",
    "dataset['Sp. Atk'] = dataset['Sp. Atk'].replace(np.NaN,dataset['Sp. Atk'].mean())\n",
    "# print(dataset['Sp. Def'].mean())\n",
    "# print(dataset['Sp. Def'].tail())\n",
    "\n",
    "dataset['Sp. Def'] = dataset['Sp. Def'].replace(np.NaN,dataset['Sp. Def'].mean())\n",
    "\n",
    "# print(dataset['Speed'].mean())\n",
    "# print(dataset['Speed'].tail())\n",
    "\n",
    "dataset['Speed'] = dataset['Speed'].replace(np.NaN,dataset['Speed'].mean())\n",
    "\n",
    "print(dataset['Speed'])\n",
    "\n",
    "print(dataset.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "_uuid": "97c807378ea7b2145f50344806552fdb86813a6f"
   },
   "outputs": [],
   "source": [
    "## Categorical NaN\n",
    "## Handling of Categorical data is quite tricker and we got to be careful about it's pre-processing techniques.There are multiple ways fro different use-cases\n",
    "## Our dataset has following features to be categorical\n",
    "## Features:[ Type 1, Type 2, Generation, Legendary]\n",
    "## The two major ways are the following\n",
    "\n",
    "# 1. Replacing the missing values with frequent occurence of value in that column.\n",
    "\n",
    "\n",
    "dataset['Type 2'].value_counts()\n",
    "def replace_most_common(x):\n",
    "    if pd.isnull(x):\n",
    "        return most_common\n",
    "    else:\n",
    "        return x\n",
    "dataset1 = dataset['Type 2'].map(replace_most_common)\n",
    "\n",
    "## This is just for understanding but we got to go with different approach.\n",
    "## If you run this cell, it may affect below ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "2307f90d324fe592fca10ddd42e058e9d985a993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      318\n",
      "1      405\n",
      "2      525\n",
      "3      625\n",
      "4      309\n",
      "5      405\n",
      "6      534\n",
      "7      634\n",
      "8      634\n",
      "9      314\n",
      "10     405\n",
      "11     530\n",
      "12     630\n",
      "13     195\n",
      "14     205\n",
      "15     395\n",
      "16     195\n",
      "17     205\n",
      "18     395\n",
      "19     495\n",
      "20     251\n",
      "21     349\n",
      "22     479\n",
      "23     579\n",
      "24     253\n",
      "25     413\n",
      "26     262\n",
      "27     442\n",
      "28     288\n",
      "29     438\n",
      "      ... \n",
      "770    525\n",
      "771    500\n",
      "772    431\n",
      "773    500\n",
      "774    300\n",
      "775    452\n",
      "776    600\n",
      "777    470\n",
      "778    309\n",
      "779    474\n",
      "780    335\n",
      "781    335\n",
      "782    335\n",
      "783    335\n",
      "784    494\n",
      "785    494\n",
      "786    494\n",
      "787    494\n",
      "788    304\n",
      "789    514\n",
      "790    245\n",
      "791    535\n",
      "792    680\n",
      "793    680\n",
      "794    600\n",
      "795    600\n",
      "796    700\n",
      "797    600\n",
      "798    680\n",
      "799    600\n",
      "Name: Total, Length: 800, dtype: int64\n",
      "[['Grass' 'Poison' 1 False]\n",
      " ['Grass' 'Poison' 1 False]\n",
      " ['Grass' 'Poison' 1 False]\n",
      " ...\n",
      " ['Psychic' 'Ghost' 6 True]\n",
      " ['Psychic' 'Dark' 6 True]\n",
      " ['Fire' 'Water' 6 True]]\n",
      "Index([u'Type 1', u'Type 2', u'Total', u'HP', u'Attack', u'Defense',\n",
      "       u'Sp. Atk', u'Sp. Def', u'Speed', u'Generation', u'Legendary'],\n",
      "      dtype='object')\n",
      "Type 1        False\n",
      "Type 2        False\n",
      "Total         False\n",
      "HP            False\n",
      "Attack        False\n",
      "Defense       False\n",
      "Sp. Atk       False\n",
      "Sp. Def       False\n",
      "Speed         False\n",
      "Generation    False\n",
      "Legendary     False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "## 2. Label Encoding \n",
    "##  LabelEncoder encode labels with a value between 0 and n_classes-1 where n is the number of distinct labels. If a label repeats it assigns the same value to as assigned earlier.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "dataset1 = dataset\n",
    "ds = dataset1[['Type 1','Type 2','Generation','Legendary']]\n",
    "print(dataset['Total'])\n",
    "#print(ds)\n",
    "X = ds.iloc[:,:4].values\n",
    "print(X)\n",
    "#print(dataset.tail())\n",
    "\n",
    "#X[:,0]=label_encoder.fit_transform(X[:,0])\n",
    "#print(X)\n",
    "X[:,1]=label_encoder.fit_transform(X[:,1].astype(str))\n",
    "X[:,2]=label_encoder.fit_transform(X[:,2])\n",
    "X[:,3]=label_encoder.fit_transform(X[:,3])\n",
    "\n",
    "##print(X[:,1])\n",
    " \n",
    "columns = ['Type 1','Type 2','Total','HP','Attack','Defense','Sp. Atk','Sp. Def','Speed','Generation','Legendary']\n",
    "\n",
    "Type1 = pd.DataFrame(X[:,0])\n",
    "Type2 = pd.DataFrame(X[:,1])\n",
    "Total = pd.DataFrame(dataset['Total'])\n",
    "HP = pd.DataFrame(dataset['HP'])\n",
    "Attack = pd.DataFrame(dataset['Attack'])\n",
    "Defense = pd.DataFrame(dataset['Defense'])\n",
    "SpAtk = pd.DataFrame(dataset['Sp. Atk'])\n",
    "SpDef= pd.DataFrame(dataset['Sp. Def'])\n",
    "Speed= pd.DataFrame(dataset['Speed'])\n",
    "Generation= pd.DataFrame(X[:,2])\n",
    "Legendary= pd.DataFrame(X[:,3])\n",
    "\n",
    "encoded_dataset = pd.DataFrame()\n",
    "encoded_dataset = pd.concat([encoded_dataset,Type1,Type2,Total,HP,Attack,Defense,SpAtk,SpDef,Speed,Generation,Legendary],axis =1)\n",
    "encoded_dataset.columns = columns\n",
    "print(encoded_dataset.columns)\n",
    "\n",
    "\n",
    "print(encoded_dataset.isna().any())\n",
    "## The problem here is, since there are different numbers in the same column, \n",
    "## the model will misunderstand the data to be in some kind of order, 0 < 1 < 2. But this isnâ€™t the case at all. \n",
    "## To overcome this problem, we use One Hot Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4fd6c5bd2ed2bd0e40bc8dad3b6f7ac483a0f81f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1.0\n",
      "  (0, 0)\t1.0\n",
      "  (1, 6)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 6)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 6)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 6)\t1.0\n",
      "  (4, 0)\t1.0\n",
      "  (5, 6)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 6)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 6)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 6)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 6)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 6)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 6)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (12, 6)\t1.0\n",
      "  :\t:\n",
      "  (775, 8)\t18.0\n",
      "  (776, 8)\t18.0\n",
      "  (777, 8)\t4.0\n",
      "  (778, 8)\t9.0\n",
      "  (779, 8)\t9.0\n",
      "  (780, 8)\t9.0\n",
      "  (781, 8)\t9.0\n",
      "  (782, 8)\t9.0\n",
      "  (783, 8)\t9.0\n",
      "  (784, 8)\t9.0\n",
      "  (785, 8)\t9.0\n",
      "  (786, 8)\t9.0\n",
      "  (787, 8)\t9.0\n",
      "  (788, 8)\t18.0\n",
      "  (789, 8)\t18.0\n",
      "  (790, 8)\t2.0\n",
      "  (791, 8)\t2.0\n",
      "  (792, 8)\t18.0\n",
      "  (793, 8)\t7.0\n",
      "  (794, 8)\t10.0\n",
      "  (795, 8)\t4.0\n",
      "  (796, 8)\t4.0\n",
      "  (797, 8)\t8.0\n",
      "  (798, 8)\t1.0\n",
      "  (799, 8)\t17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/AI/lib/python2.7/site-packages/sklearn/preprocessing/_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## 3. One hot encoding - It is the most widespread approach, and it works very well unless your categorical variable takes on a large number of values \n",
    "## (i.e. you generally won't it for variables taking more than 15 different values. It'd be a poor choice in some cases with fewer values, though that varies.)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(categorical_features=[1,-1])\n",
    "X = onehotencoder.fit_transform(X[:,1:])\n",
    "print(X)\n",
    "\n",
    "## In this example, the categories type 2 and legendary are one-hot encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6dda47efc6c77396ef64175c18cb6acc108b988b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Bug', u'Dark', u'Dragon', u'Electric', u'Fairy', u'Fighting', u'Fire',\n",
      "       u'Flying', u'Ghost', u'Grass', u'Ground', u'Ice', u'Normal', u'Poison',\n",
      "       u'Psychic', u'Rock', u'Steel', u'Water'],\n",
      "      dtype='object')\n",
      "       #                       Name Type 2  Total   HP  Attack  Defense  \\\n",
      "0      1                  Bulbasaur     13    318   45      49       49   \n",
      "1      2                    Ivysaur     13    405   60      62       63   \n",
      "2      3                   Venusaur     13    525   80      82       83   \n",
      "3      3      VenusaurMega Venusaur     13    625   80     100      123   \n",
      "4      4                 Charmander     18    309   39      52       43   \n",
      "5      5                 Charmeleon     18    405   58      64       58   \n",
      "6      6                  Charizard      7    534   78      84       78   \n",
      "7      6  CharizardMega Charizard X      2    634   78     130      111   \n",
      "8      6  CharizardMega Charizard Y      7    634   78     104       78   \n",
      "9      7                   Squirtle     18    314   44      48       65   \n",
      "10     8                  Wartortle     18    405   59      63       80   \n",
      "11     9                  Blastoise     18    530   79      83      100   \n",
      "12     9    BlastoiseMega Blastoise     18    630   79     103      120   \n",
      "13    10                   Caterpie     18    195   45      30       35   \n",
      "14    11                    Metapod     18    205   50      20       55   \n",
      "15    12                 Butterfree      7    395   60      45       50   \n",
      "16    13                     Weedle     13    195   40      35       30   \n",
      "17    14                     Kakuna     13    205   45      25       50   \n",
      "18    15                   Beedrill     13    395   65      90       40   \n",
      "19    15      BeedrillMega Beedrill     13    495   65     150       40   \n",
      "20    16                     Pidgey      7    251   40      45       40   \n",
      "21    17                  Pidgeotto      7    349   63      60       55   \n",
      "22    18                    Pidgeot      7    479   83      80       75   \n",
      "23    18        PidgeotMega Pidgeot      7    579   83      80       80   \n",
      "24    19                    Rattata     18    253   30      56       35   \n",
      "25    20                   Raticate     18    413   55      81       60   \n",
      "26    21                    Spearow      7    262   40      60       30   \n",
      "27    22                     Fearow      7    442   65      90       65   \n",
      "28    23                      Ekans     18    288   35      60       44   \n",
      "29    24                      Arbok     18    438   60      85       69   \n",
      "..   ...                        ...    ...    ...  ...     ...      ...   \n",
      "770  700                    Sylveon     18    525   95      65       65   \n",
      "771  701                   Hawlucha      7    500   78      92       75   \n",
      "772  702                    Dedenne      4    431   67      58       57   \n",
      "773  703                    Carbink      4    500   50      50      150   \n",
      "774  704                      Goomy     18    300   45      50       35   \n",
      "775  705                    Sliggoo     18    452   68      75       53   \n",
      "776  706                     Goodra     18    600   90     100       70   \n",
      "777  707                     Klefki      4    470   57      80       91   \n",
      "778  708                   Phantump      9    309   43      70       48   \n",
      "779  709                  Trevenant      9    474   85     110       76   \n",
      "780  710      PumpkabooAverage Size      9    335   49      66       70   \n",
      "781  710        PumpkabooSmall Size      9    335   44      66       70   \n",
      "782  710        PumpkabooLarge Size      9    335   54      66       70   \n",
      "783  710        PumpkabooSuper Size      9    335   59      66       70   \n",
      "784  711      GourgeistAverage Size      9    494   65      90      122   \n",
      "785  711        GourgeistSmall Size      9    494   55      85      122   \n",
      "786  711        GourgeistLarge Size      9    494   75      95      122   \n",
      "787  711        GourgeistSuper Size      9    494   85     100      122   \n",
      "788  712                   Bergmite     18    304   55      69       85   \n",
      "789  713                    Avalugg     18    514   95     117      184   \n",
      "790  714                     Noibat      2    245   40      30       35   \n",
      "791  715                    Noivern      2    535   85      70       80   \n",
      "792  716                    Xerneas     18    680  126     131       95   \n",
      "793  717                    Yveltal      7    680  126     131       95   \n",
      "794  718           Zygarde50% Forme     10    600  108     100      121   \n",
      "795  719                    Diancie      4    600   50     100      150   \n",
      "796  719        DiancieMega Diancie      4    700   50     160      110   \n",
      "797  720        HoopaHoopa Confined      8    600   80     110       60   \n",
      "798  720         HoopaHoopa Unbound      1    680   80     160       60   \n",
      "799  721                  Volcanion     17    600   80     110      120   \n",
      "\n",
      "     Sp. Atk  Sp. Def  Speed  ... Ghost Grass  Ground  Ice  Normal  Poison  \\\n",
      "0         65       65     45  ...     0     1       0    0       0       0   \n",
      "1         80       80     60  ...     0     1       0    0       0       0   \n",
      "2        100      100     80  ...     0     1       0    0       0       0   \n",
      "3        122      120     80  ...     0     1       0    0       0       0   \n",
      "4         60       50     65  ...     0     0       0    0       0       0   \n",
      "5         80       65     80  ...     0     0       0    0       0       0   \n",
      "6        109       85    100  ...     0     0       0    0       0       0   \n",
      "7        130       85    100  ...     0     0       0    0       0       0   \n",
      "8        159      115    100  ...     0     0       0    0       0       0   \n",
      "9         50       64     43  ...     0     0       0    0       0       0   \n",
      "10        65       80     58  ...     0     0       0    0       0       0   \n",
      "11        85      105     78  ...     0     0       0    0       0       0   \n",
      "12       135      115     78  ...     0     0       0    0       0       0   \n",
      "13        20       20     45  ...     0     0       0    0       0       0   \n",
      "14        25       25     30  ...     0     0       0    0       0       0   \n",
      "15        90       80     70  ...     0     0       0    0       0       0   \n",
      "16        20       20     50  ...     0     0       0    0       0       0   \n",
      "17        25       25     35  ...     0     0       0    0       0       0   \n",
      "18        45       80     75  ...     0     0       0    0       0       0   \n",
      "19        15       80    145  ...     0     0       0    0       0       0   \n",
      "20        35       35     56  ...     0     0       0    0       1       0   \n",
      "21        50       50     71  ...     0     0       0    0       1       0   \n",
      "22        70       70    101  ...     0     0       0    0       1       0   \n",
      "23       135       80    121  ...     0     0       0    0       1       0   \n",
      "24        25       35     72  ...     0     0       0    0       1       0   \n",
      "25        50       70     97  ...     0     0       0    0       1       0   \n",
      "26        31       31     70  ...     0     0       0    0       1       0   \n",
      "27        61       61    100  ...     0     0       0    0       1       0   \n",
      "28        40       54     55  ...     0     0       0    0       0       1   \n",
      "29        65       79     80  ...     0     0       0    0       0       1   \n",
      "..       ...      ...    ...  ...   ...   ...     ...  ...     ...     ...   \n",
      "770      110      130     60  ...     0     0       0    0       0       0   \n",
      "771       74       63    118  ...     0     0       0    0       0       0   \n",
      "772       81       67    101  ...     0     0       0    0       0       0   \n",
      "773       50      150     50  ...     0     0       0    0       0       0   \n",
      "774       55       75     40  ...     0     0       0    0       0       0   \n",
      "775       83      113     60  ...     0     0       0    0       0       0   \n",
      "776      110      150     80  ...     0     0       0    0       0       0   \n",
      "777       80       87     75  ...     0     0       0    0       0       0   \n",
      "778       50       60     38  ...     1     0       0    0       0       0   \n",
      "779       65       82     56  ...     1     0       0    0       0       0   \n",
      "780       44       55     51  ...     1     0       0    0       0       0   \n",
      "781       44       55     56  ...     1     0       0    0       0       0   \n",
      "782       44       55     46  ...     1     0       0    0       0       0   \n",
      "783       44       55     41  ...     1     0       0    0       0       0   \n",
      "784       58       75     84  ...     1     0       0    0       0       0   \n",
      "785       58       75     99  ...     1     0       0    0       0       0   \n",
      "786       58       75     69  ...     1     0       0    0       0       0   \n",
      "787       58       75     54  ...     1     0       0    0       0       0   \n",
      "788       32       35     28  ...     0     0       0    1       0       0   \n",
      "789       44       46     28  ...     0     0       0    1       0       0   \n",
      "790       45       40     55  ...     0     0       0    0       0       0   \n",
      "791       97       80    123  ...     0     0       0    0       0       0   \n",
      "792      131       98     99  ...     0     0       0    0       0       0   \n",
      "793      131       98     99  ...     0     0       0    0       0       0   \n",
      "794       81       95     95  ...     0     0       0    0       0       0   \n",
      "795      100      150     50  ...     0     0       0    0       0       0   \n",
      "796      160      110    110  ...     0     0       0    0       0       0   \n",
      "797      150      130     70  ...     0     0       0    0       0       0   \n",
      "798      170      130     80  ...     0     0       0    0       0       0   \n",
      "799      130       90     70  ...     0     0       0    0       0       0   \n",
      "\n",
      "     Psychic  Rock  Steel  Water  \n",
      "0          0     0      0      0  \n",
      "1          0     0      0      0  \n",
      "2          0     0      0      0  \n",
      "3          0     0      0      0  \n",
      "4          0     0      0      0  \n",
      "5          0     0      0      0  \n",
      "6          0     0      0      0  \n",
      "7          0     0      0      0  \n",
      "8          0     0      0      0  \n",
      "9          0     0      0      1  \n",
      "10         0     0      0      1  \n",
      "11         0     0      0      1  \n",
      "12         0     0      0      1  \n",
      "13         0     0      0      0  \n",
      "14         0     0      0      0  \n",
      "15         0     0      0      0  \n",
      "16         0     0      0      0  \n",
      "17         0     0      0      0  \n",
      "18         0     0      0      0  \n",
      "19         0     0      0      0  \n",
      "20         0     0      0      0  \n",
      "21         0     0      0      0  \n",
      "22         0     0      0      0  \n",
      "23         0     0      0      0  \n",
      "24         0     0      0      0  \n",
      "25         0     0      0      0  \n",
      "26         0     0      0      0  \n",
      "27         0     0      0      0  \n",
      "28         0     0      0      0  \n",
      "29         0     0      0      0  \n",
      "..       ...   ...    ...    ...  \n",
      "770        0     0      0      0  \n",
      "771        0     0      0      0  \n",
      "772        0     0      0      0  \n",
      "773        0     1      0      0  \n",
      "774        0     0      0      0  \n",
      "775        0     0      0      0  \n",
      "776        0     0      0      0  \n",
      "777        0     0      1      0  \n",
      "778        0     0      0      0  \n",
      "779        0     0      0      0  \n",
      "780        0     0      0      0  \n",
      "781        0     0      0      0  \n",
      "782        0     0      0      0  \n",
      "783        0     0      0      0  \n",
      "784        0     0      0      0  \n",
      "785        0     0      0      0  \n",
      "786        0     0      0      0  \n",
      "787        0     0      0      0  \n",
      "788        0     0      0      0  \n",
      "789        0     0      0      0  \n",
      "790        0     0      0      0  \n",
      "791        0     0      0      0  \n",
      "792        0     0      0      0  \n",
      "793        0     0      0      0  \n",
      "794        0     0      0      0  \n",
      "795        0     1      0      0  \n",
      "796        0     1      0      0  \n",
      "797        1     0      0      0  \n",
      "798        1     0      0      0  \n",
      "799        0     0      0      0  \n",
      "\n",
      "[800 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "## 4. Creating dummies is another method of handling categorical data and it is somewhat similar to one hot encoding \n",
    "## Dummy Variables is one that takes the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome.\n",
    "## Number of columns = number of category values\n",
    "\n",
    "dummy = pd.get_dummies(encoded_dataset['Type 1'])\n",
    "print(dummy.columns)\n",
    "tdataset = dataset[['#', 'Name']]\n",
    "transformed_dataset = pd.concat([tdataset,encoded_dataset],axis = 1)\n",
    "transformed_dataset = pd.concat([transformed_dataset,dummy],axis =1)\n",
    "transformed_dataset = transformed_dataset.drop(['Type 1'],axis = 1)\n",
    "\n",
    "print(transformed_dataset)\n",
    "\n",
    "\n",
    "\n",
    "## 5. Sometimes, we use KNN Imputation(for Categorical variables): In this method of imputation, \n",
    "## the missing values of an attribute are imputed using the given number of attributes that are most similar to the attribute whose values are missing. \n",
    "## The similarity of two attributes is determined using a distance function, but we are going to stop our experiment only with dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "4d6f97fd37db1ecb1fd19e5a992380741e97161a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'#', u'Name', u'Type 2', u'Total', u'HP', u'Attack', u'Defense',\n",
      "       u'Sp. Atk', u'Sp. Def', u'Speed', u'Generation', u'Legendary', u'Bug',\n",
      "       u'Dark', u'Dragon', u'Electric', u'Fairy', u'Fighting', u'Fire',\n",
      "       u'Flying', u'Ghost', u'Grass', u'Ground', u'Ice', u'Normal', u'Poison',\n",
      "       u'Psychic', u'Rock', u'Steel', u'Water'],\n",
      "      dtype='object')\n",
      "#             False\n",
      "Total         False\n",
      "HP            False\n",
      "Attack        False\n",
      "Defense       False\n",
      "Sp. Atk       False\n",
      "Sp. Def       False\n",
      "Speed         False\n",
      "Generation    False\n",
      "Legendary     False\n",
      "Bug           False\n",
      "Dark          False\n",
      "Dragon        False\n",
      "Electric      False\n",
      "Fairy         False\n",
      "Fighting      False\n",
      "Fire          False\n",
      "Flying        False\n",
      "Ghost         False\n",
      "Grass         False\n",
      "Ground        False\n",
      "Ice           False\n",
      "Normal        False\n",
      "Poison        False\n",
      "Psychic       False\n",
      "Rock          False\n",
      "Steel         False\n",
      "Water         False\n",
      "dtype: bool\n",
      "Type 2    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# 'Bug', 'Dark', 'Dragon', 'Electric', 'Fairy', 'Fighting', 'Fire',\n",
    "#        'Flying', 'Ghost', 'Grass', 'Ground', 'Ice', 'Normal', 'Poison',\n",
    "#        'Psychic', 'Rock', 'Steel', 'Water'\n",
    "print(transformed_dataset.columns)\n",
    "\n",
    "## Eliminating the name columns as we have '#' \n",
    "X = transformed_dataset[['#','Total','HP','Attack','Defense','Sp. Atk',\n",
    "       'Sp. Def', 'Speed', 'Generation','Legendary','Bug', 'Dark', 'Dragon',\n",
    "       'Electric', 'Fairy', 'Fighting', 'Fire', 'Flying', 'Ghost', 'Grass',\n",
    "       'Ground', 'Ice', 'Normal', 'Poison', 'Psychic', 'Rock', 'Steel',\n",
    "       'Water']]\n",
    "print(X.isna().any())\n",
    "y = transformed_dataset[['Type 2']]\n",
    "y = y.astype('float')\n",
    "print(y.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "ffcfa7b2979d9c172317c9c633e9f287f105d08d"
   },
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "468ee73bde36282ebed01b0aae2bea6149ea7c8b"
   },
   "outputs": [],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "4897258ffe1ae4e990b423db3695126db4231325",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.49166666666666664)\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1]\n",
      " [  0   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0\n",
      "    5]\n",
      " [  0   0   0   0   0   0   0   2   0   1   0   0   0   0   0   0   0   0\n",
      "    5]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    4]\n",
      " [  0   0   0   0   0   1   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "    7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    2]\n",
      " [  0   0   0   0   0   0   0   8   0   0   0   0   0   0   0   0   1   0\n",
      "   18]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   1   0\n",
      "    5]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   1   0\n",
      "    8]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1]\n",
      " [  0   0   0   0   0   0   0   1   0   0   0   0   0   1   0   0   0   0\n",
      "    9]\n",
      " [  0   0   0   0   0   0   0   2   0   0   0   0   0   0   2   0   0   0\n",
      "    7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0\n",
      "    4]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0\n",
      "    6]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    6]\n",
      " [  0   0   1   0   0   0   0   7   0   2   0   0   0   0   0   2   1   0\n",
      "  101]]\n",
      "('ACC', matrix([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "conf_mat=np.matrix(conf_mat)\n",
    "FP = conf_mat.sum(axis=0) - np.diag(conf_mat)  \n",
    "FN = conf_mat.sum(axis=1) - np.diag(conf_mat)\n",
    "TP = np.diag(conf_mat)\n",
    "TN = conf_mat.sum() - (FP + FN + TP)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('ACC',ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "a810da36730504177f0741e111e290aa22fe0e7d"
   },
   "outputs": [],
   "source": [
    "## This exercise of work is for demonstrating pre-processing techniques only, The model can give around 50% accuracy for now.\n",
    "## We got to apply some more data to make it improve it's accuracy as well hyper tuning of parameters in the algorithm.\n",
    "\n",
    "## The overall problem that the solution covers is to identify type 2 of the pokemon using other features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
