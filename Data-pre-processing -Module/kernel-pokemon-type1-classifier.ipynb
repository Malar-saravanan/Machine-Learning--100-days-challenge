{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Importing the required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')",
      "execution_count": 68,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7af7cef5d3d3ef24b4df12fa13b429aaf93a0c1e"
      },
      "cell_type": "code",
      "source": "## Step 1: Importing data from source\ndataset = pd.read_csv(\"../input/Pokemon.csv\")",
      "execution_count": 69,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65947793ca800479b0743ef54a1dbb1f4f6abe1c"
      },
      "cell_type": "code",
      "source": "## Analyzing the structure and aspects of data\nprint(dataset.head(5))\nprint(dataset.shape)\nprint(dataset.index)\nprint(dataset.columns)",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": "   #                   Name Type 1    ...     Speed  Generation  Legendary\n0  1              Bulbasaur  Grass    ...        45           1      False\n1  2                Ivysaur  Grass    ...        60           1      False\n2  3               Venusaur  Grass    ...        80           1      False\n3  3  VenusaurMega Venusaur  Grass    ...        80           1      False\n4  4             Charmander   Fire    ...        65           1      False\n\n[5 rows x 13 columns]\n(800, 13)\nRangeIndex(start=0, stop=800, step=1)\nIndex(['#', 'Name', 'Type 1', 'Type 2', 'Total', 'HP', 'Attack', 'Defense',\n       'Sp. Atk', 'Sp. Def', 'Speed', 'Generation', 'Legendary'],\n      dtype='object')\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f2e3760e741073010ab2550f81e3d2734e33597"
      },
      "cell_type": "code",
      "source": "## Since the dataset is quite minimal, Let's try the alternative method for handling missing values\n\n## Technique 2 - This can be applied on features which has numerical data like year, values etc. This is an approximation which adds variance to the dataset but can avoid loss of data\n## It's a standard technique and for this dataset we have mix of both numerical and categorical data.\n\n## Numerical NaN \n## In the given dataset below are the features with numerical values\n\n## Features :[Total,HP,Attack,Defense,Sp.Atk,Sp.Def,Speed]\n## Note: Generation is a numerical value but those values are categorical so we are not considering it.\n\n# print(dataset['Total'].mean())\n# print(dataset['Total'].tail())\n\ndataset['Total']= dataset['Total'].fillna(dataset['Total'].mean())\n\n\n\n## Similar technique to be adopted for other numerical columns\n\n# print(dataset['HP'].mean())\n# print(dataset['HP'].tail())\n\ndataset['HP']= dataset['HP'].replace(np.NaN,dataset['HP'].mean())\n\n# print(dataset['Attack'].mean())\n# print(dataset['Attack'].tail())\n\ndataset['Attack'] = dataset['Attack'].replace(np.NaN,dataset['Attack'].mean())\n# print(dataset['Defense'].mean())\n# print(dataset['Defense'].tail())\n\ndataset['Defense'] = dataset['Defense'].replace(np.NaN,dataset['Defense'].mean())\n# print(dataset['Sp. Atk'].mean())\n# print(dataset['Sp. Atk'].tail())\n\ndataset['Sp. Atk'] = dataset['Sp. Atk'].replace(np.NaN,dataset['Sp. Atk'].mean())\n# print(dataset['Sp. Def'].mean())\n# print(dataset['Sp. Def'].tail())\n\ndataset['Sp. Def'] = dataset['Sp. Def'].replace(np.NaN,dataset['Sp. Def'].mean())\n\n# print(dataset['Speed'].mean())\n# print(dataset['Speed'].tail())\n\ndataset['Speed'] = dataset['Speed'].replace(np.NaN,dataset['Speed'].mean())\n\nprint(dataset['Speed'])\nprint(dataset.isna().any())\n",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0       45\n1       60\n2       80\n3       80\n4       65\n5       80\n6      100\n7      100\n8      100\n9       43\n10      58\n11      78\n12      78\n13      45\n14      30\n15      70\n16      50\n17      35\n18      75\n19     145\n20      56\n21      71\n22     101\n23     121\n24      72\n25      97\n26      70\n27     100\n28      55\n29      80\n      ... \n770     60\n771    118\n772    101\n773     50\n774     40\n775     60\n776     80\n777     75\n778     38\n779     56\n780     51\n781     56\n782     46\n783     41\n784     84\n785     99\n786     69\n787     54\n788     28\n789     28\n790     55\n791    123\n792     99\n793     99\n794     95\n795     50\n796    110\n797     70\n798     80\n799     70\nName: Speed, Length: 800, dtype: int64\n#             False\nName          False\nType 1        False\nType 2         True\nTotal         False\nHP            False\nAttack        False\nDefense       False\nSp. Atk       False\nSp. Def       False\nSpeed         False\nGeneration    False\nLegendary     False\ndtype: bool\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2307f90d324fe592fca10ddd42e058e9d985a993"
      },
      "cell_type": "code",
      "source": "## 2. Label Encoding \n##  LabelEncoder encode labels with a value between 0 and n_classes-1 where n is the number of distinct labels. If a label repeats it assigns the same value to as assigned earlier.\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\ndataset1 = dataset\nds = dataset1[['Type 1','Type 2','Generation','Legendary']]\nprint(dataset['Total'])\n#print(ds)\nX = ds.iloc[:,:4].values\nprint(X)\n#print(dataset.tail())\n\nX[:,0]=label_encoder.fit_transform(X[:,0])\n#print(X)\n#X[:,1]=label_encoder.fit_transform(X[:,1].astype(str))\nX[:,2]=label_encoder.fit_transform(X[:,2])\nX[:,3]=label_encoder.fit_transform(X[:,3])\n\n##print(X[:,1])\n \ncolumns = ['Type 1','Type 2','Total','HP','Attack','Defense','Sp. Atk','Sp. Def','Speed','Generation','Legendary']\n\nType1 = pd.DataFrame(X[:,0])\nType2 = pd.DataFrame(X[:,1])\nTotal = pd.DataFrame(dataset['Total'])\nHP = pd.DataFrame(dataset['HP'])\nAttack = pd.DataFrame(dataset['Attack'])\nDefense = pd.DataFrame(dataset['Defense'])\nSpAtk = pd.DataFrame(dataset['Sp. Atk'])\nSpDef= pd.DataFrame(dataset['Sp. Def'])\nSpeed= pd.DataFrame(dataset['Speed'])\nGeneration= pd.DataFrame(X[:,2])\nLegendary= pd.DataFrame(X[:,3])\n\nencoded_dataset = pd.DataFrame()\nencoded_dataset = pd.concat([encoded_dataset,Type1,Type2,Total,HP,Attack,Defense,SpAtk,SpDef,Speed,Generation,Legendary],axis =1)\nencoded_dataset.columns = columns\nprint(encoded_dataset.columns)\n## The problem here is, since there are different numbers in the same column, \n## the model will misunderstand the data to be in some kind of order, 0 < 1 < 2. But this isnâ€™t the case at all. \n## To overcome this problem, we use One Hot Encoder.",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0      318\n1      405\n2      525\n3      625\n4      309\n5      405\n6      534\n7      634\n8      634\n9      314\n10     405\n11     530\n12     630\n13     195\n14     205\n15     395\n16     195\n17     205\n18     395\n19     495\n20     251\n21     349\n22     479\n23     579\n24     253\n25     413\n26     262\n27     442\n28     288\n29     438\n      ... \n770    525\n771    500\n772    431\n773    500\n774    300\n775    452\n776    600\n777    470\n778    309\n779    474\n780    335\n781    335\n782    335\n783    335\n784    494\n785    494\n786    494\n787    494\n788    304\n789    514\n790    245\n791    535\n792    680\n793    680\n794    600\n795    600\n796    700\n797    600\n798    680\n799    600\nName: Total, Length: 800, dtype: int64\n[['Grass' 'Poison' 1 False]\n ['Grass' 'Poison' 1 False]\n ['Grass' 'Poison' 1 False]\n ...\n ['Psychic' 'Ghost' 6 True]\n ['Psychic' 'Dark' 6 True]\n ['Fire' 'Water' 6 True]]\nIndex(['Type 1', 'Type 2', 'Total', 'HP', 'Attack', 'Defense', 'Sp. Atk',\n       'Sp. Def', 'Speed', 'Generation', 'Legendary'],\n      dtype='object')\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6dda47efc6c77396ef64175c18cb6acc108b988b"
      },
      "cell_type": "code",
      "source": "## 4. Creating dummies is another method of handling categorical data and it is somewhat similar to one hot encoding \n## Dummy Variables is one that takes the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome.\n## Number of columns = number of category values\n\ndummy = pd.get_dummies(encoded_dataset['Type 2'])\nprint(dummy.columns)\ntdataset = dataset[['#', 'Name']]\ntransformed_dataset = pd.concat([tdataset,encoded_dataset],axis = 1)\ntransformed_dataset = pd.concat([transformed_dataset,dummy],axis =1)\ntransformed_dataset = transformed_dataset.drop(['Type 2'],axis = 1)\n\nprint(transformed_dataset)\n\n\n\n## 5. Sometimes, we use KNN Imputation(for Categorical variables): In this method of imputation, \n## the missing values of an attribute are imputed using the given number of attributes that are most similar to the attribute whose values are missing. \n## The similarity of two attributes is determined using a distance function, but we are going to stop our experiment only with dummies.",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Index(['Bug', 'Dark', 'Dragon', 'Electric', 'Fairy', 'Fighting', 'Fire',\n       'Flying', 'Ghost', 'Grass', 'Ground', 'Ice', 'Normal', 'Poison',\n       'Psychic', 'Rock', 'Steel', 'Water'],\n      dtype='object')\n       #                       Name Type 1  ...    Rock  Steel  Water\n0      1                  Bulbasaur      9  ...       0      0      0\n1      2                    Ivysaur      9  ...       0      0      0\n2      3                   Venusaur      9  ...       0      0      0\n3      3      VenusaurMega Venusaur      9  ...       0      0      0\n4      4                 Charmander      6  ...       0      0      0\n5      5                 Charmeleon      6  ...       0      0      0\n6      6                  Charizard      6  ...       0      0      0\n7      6  CharizardMega Charizard X      6  ...       0      0      0\n8      6  CharizardMega Charizard Y      6  ...       0      0      0\n9      7                   Squirtle     17  ...       0      0      0\n10     8                  Wartortle     17  ...       0      0      0\n11     9                  Blastoise     17  ...       0      0      0\n12     9    BlastoiseMega Blastoise     17  ...       0      0      0\n13    10                   Caterpie      0  ...       0      0      0\n14    11                    Metapod      0  ...       0      0      0\n15    12                 Butterfree      0  ...       0      0      0\n16    13                     Weedle      0  ...       0      0      0\n17    14                     Kakuna      0  ...       0      0      0\n18    15                   Beedrill      0  ...       0      0      0\n19    15      BeedrillMega Beedrill      0  ...       0      0      0\n20    16                     Pidgey     12  ...       0      0      0\n21    17                  Pidgeotto     12  ...       0      0      0\n22    18                    Pidgeot     12  ...       0      0      0\n23    18        PidgeotMega Pidgeot     12  ...       0      0      0\n24    19                    Rattata     12  ...       0      0      0\n25    20                   Raticate     12  ...       0      0      0\n26    21                    Spearow     12  ...       0      0      0\n27    22                     Fearow     12  ...       0      0      0\n28    23                      Ekans     13  ...       0      0      0\n29    24                      Arbok     13  ...       0      0      0\n..   ...                        ...    ...  ...     ...    ...    ...\n770  700                    Sylveon      4  ...       0      0      0\n771  701                   Hawlucha      5  ...       0      0      0\n772  702                    Dedenne      3  ...       0      0      0\n773  703                    Carbink     15  ...       0      0      0\n774  704                      Goomy      2  ...       0      0      0\n775  705                    Sliggoo      2  ...       0      0      0\n776  706                     Goodra      2  ...       0      0      0\n777  707                     Klefki     16  ...       0      0      0\n778  708                   Phantump      8  ...       0      0      0\n779  709                  Trevenant      8  ...       0      0      0\n780  710      PumpkabooAverage Size      8  ...       0      0      0\n781  710        PumpkabooSmall Size      8  ...       0      0      0\n782  710        PumpkabooLarge Size      8  ...       0      0      0\n783  710        PumpkabooSuper Size      8  ...       0      0      0\n784  711      GourgeistAverage Size      8  ...       0      0      0\n785  711        GourgeistSmall Size      8  ...       0      0      0\n786  711        GourgeistLarge Size      8  ...       0      0      0\n787  711        GourgeistSuper Size      8  ...       0      0      0\n788  712                   Bergmite     11  ...       0      0      0\n789  713                    Avalugg     11  ...       0      0      0\n790  714                     Noibat      7  ...       0      0      0\n791  715                    Noivern      7  ...       0      0      0\n792  716                    Xerneas      4  ...       0      0      0\n793  717                    Yveltal      1  ...       0      0      0\n794  718           Zygarde50% Forme      2  ...       0      0      0\n795  719                    Diancie     15  ...       0      0      0\n796  719        DiancieMega Diancie     15  ...       0      0      0\n797  720        HoopaHoopa Confined     14  ...       0      0      0\n798  720         HoopaHoopa Unbound     14  ...       0      0      0\n799  721                  Volcanion      6  ...       0      0      1\n\n[800 rows x 30 columns]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d6f97fd37db1ecb1fd19e5a992380741e97161a"
      },
      "cell_type": "code",
      "source": "# 'Bug', 'Dark', 'Dragon', 'Electric', 'Fairy', 'Fighting', 'Fire',\n#        'Flying', 'Ghost', 'Grass', 'Ground', 'Ice', 'Normal', 'Poison',\n#        'Psychic', 'Rock', 'Steel', 'Water'\nprint(transformed_dataset.columns)\n\n## Eliminating the name columns as we have '#' \nX = transformed_dataset[['#','Total','HP','Attack','Defense','Sp. Atk',\n       'Sp. Def', 'Speed', 'Generation','Legendary','Bug', 'Dark', 'Dragon',\n       'Electric', 'Fairy', 'Fighting', 'Fire', 'Flying', 'Ghost', 'Grass',\n       'Ground', 'Ice', 'Normal', 'Poison', 'Psychic', 'Rock', 'Steel',\n       'Water']]\ny = transformed_dataset[['Type 1']]\ny=y.astype('long')\nprint(X.isna().any())\nprint(y.isna().any())\n\nprint(np.where(y.values >= np.finfo(np.float64).max))",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Index(['#', 'Name', 'Type 1', 'Total', 'HP', 'Attack', 'Defense', 'Sp. Atk',\n       'Sp. Def', 'Speed', 'Generation', 'Legendary', 'Bug', 'Dark', 'Dragon',\n       'Electric', 'Fairy', 'Fighting', 'Fire', 'Flying', 'Ghost', 'Grass',\n       'Ground', 'Ice', 'Normal', 'Poison', 'Psychic', 'Rock', 'Steel',\n       'Water'],\n      dtype='object')\n#             False\nTotal         False\nHP            False\nAttack        False\nDefense       False\nSp. Atk       False\nSp. Def       False\nSpeed         False\nGeneration    False\nLegendary     False\nBug           False\nDark          False\nDragon        False\nElectric      False\nFairy         False\nFighting      False\nFire          False\nFlying        False\nGhost         False\nGrass         False\nGround        False\nIce           False\nNormal        False\nPoison        False\nPsychic       False\nRock          False\nSteel         False\nWater         False\ndtype: bool\nType 1    False\ndtype: bool\n(array([], dtype=int64), array([], dtype=int64))\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ffcfa7b2979d9c172317c9c633e9f287f105d08d"
      },
      "cell_type": "code",
      "source": "# Import train_test_split function\nfrom sklearn.model_selection import train_test_split\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "468ee73bde36282ebed01b0aae2bea6149ea7c8b"
      },
      "cell_type": "code",
      "source": "#Import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)",
      "execution_count": 75,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "4897258ffe1ae4e990b423db3695126db4231325"
      },
      "cell_type": "code",
      "source": "#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\n#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n\nfrom sklearn.metrics import confusion_matrix\n\nconf_mat = confusion_matrix(y_test, y_pred)\nprint(conf_mat)\n\nconf_mat=np.matrix(conf_mat)\nFP = conf_mat.sum(axis=0) - np.diag(conf_mat)  \nFN = conf_mat.sum(axis=1) - np.diag(conf_mat)\nTP = np.diag(conf_mat)\nTN = conf_mat.sum() - (FP + FN + TP)\n\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP/(TP+FN)\n# Specificity or true negative rate\nTNR = TN/(TN+FP) \n# Precision or positive predictive value\nPPV = TP/(TP+FP)\n# Negative predictive value\nNPV = TN/(TN+FN)\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\n# False negative rate\nFNR = FN/(TP+FN)\n# False discovery rate\nFDR = FP/(TP+FP)\n\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\n\nprint('ACC',ACC)",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[ 8  0  0  0  0  0  0  1  1  0  0  1  0  0  0  1  1]\n [ 1  0  0  0  1  0  1  1  0  0  0  6  0  0  0  0  1]\n [ 1  0  6  1  0  0  0  0  0  0  0  2  0  1  0  0  1]\n [ 1  0  0  2  0  0  4  0  0  0  0  1  0  1  0  0  5]\n [ 0  0  0  0  1  0  0  0  0  0  0  2  0  1  0  0  2]\n [ 0  1  0  0  0  3  0  0  1  0  1  3  1  0  0  0  0]\n [ 1  0  0  3  0  1  6  0  2  0  0  1  0  1  0  1  3]\n [ 1  0  0  1  0  0  0  3  2  0  0  1  0  1  0  0  1]\n [ 2  0  0  0  0  0  2  0  4  0  0  3  0  3  1  0  7]\n [ 2  0  0  0  0  0  0  0  0  1  1  4  0  0  2  2  2]\n [ 0  0  0  2  0  0  1  0  0  0  0  2  0  0  0  0  4]\n [ 2  0  1  0  0  1  1  0  2  0  0 18  1  0  1  0  1]\n [ 0  0  0  0  0  0  0  0  2  0  0  1  2  0  0  0  2]\n [ 0  0  1  1  0  0  0  0  1  0  0  2  0  8  0  0  4]\n [ 0  0  0  0  0  0  0  0  1  0  0  0  0  2  3  0  4]\n [ 0  1  0  1  0  0  0  0  3  0  0  0  0  0  0  0  0]\n [ 3  1  0  3  1  0  3  0  4  1  0  3  0  2  1  1 10]]\nACC [[0.92083333 0.93333333 0.9625     0.90416667 0.94166667 0.95\n  0.92083333 0.95       0.88333333 0.94583333 0.9375     0.8875\n  0.94583333 0.92916667 0.9375     0.925      0.82916667]\n [0.92916667 0.94166667 0.97083333 0.9125     0.95       0.95833333\n  0.92916667 0.95833333 0.89166667 0.95416667 0.94583333 0.89583333\n  0.95416667 0.9375     0.94583333 0.93333333 0.8375    ]\n [0.925      0.9375     0.96666667 0.90833333 0.94583333 0.95416667\n  0.925      0.95416667 0.8875     0.95       0.94166667 0.89166667\n  0.95       0.93333333 0.94166667 0.92916667 0.83333333]\n [0.91666667 0.92916667 0.95833333 0.9        0.9375     0.94583333\n  0.91666667 0.94583333 0.87916667 0.94166667 0.93333333 0.88333333\n  0.94166667 0.925      0.93333333 0.92083333 0.825     ]\n [0.95       0.9625     0.99166667 0.93333333 0.97083333 0.97916667\n  0.95       0.97916667 0.9125     0.975      0.96666667 0.91666667\n  0.975      0.95833333 0.96666667 0.95416667 0.85833333]\n [0.93333333 0.94583333 0.975      0.91666667 0.95416667 0.9625\n  0.93333333 0.9625     0.89583333 0.95833333 0.95       0.9\n  0.95833333 0.94166667 0.95       0.9375     0.84166667]\n [0.89583333 0.90833333 0.9375     0.87916667 0.91666667 0.925\n  0.89583333 0.925      0.85833333 0.92083333 0.9125     0.8625\n  0.92083333 0.90416667 0.9125     0.9        0.80416667]\n [0.93333333 0.94583333 0.975      0.91666667 0.95416667 0.9625\n  0.93333333 0.9625     0.89583333 0.95833333 0.95       0.9\n  0.95833333 0.94166667 0.95       0.9375     0.84166667]\n [0.88333333 0.89583333 0.925      0.86666667 0.90416667 0.9125\n  0.88333333 0.9125     0.84583333 0.90833333 0.9        0.85\n  0.90833333 0.89166667 0.9        0.8875     0.79166667]\n [0.91666667 0.92916667 0.95833333 0.9        0.9375     0.94583333\n  0.91666667 0.94583333 0.87916667 0.94166667 0.93333333 0.88333333\n  0.94166667 0.925      0.93333333 0.92083333 0.825     ]\n [0.9375     0.95       0.97916667 0.92083333 0.95833333 0.96666667\n  0.9375     0.96666667 0.9        0.9625     0.95416667 0.90416667\n  0.9625     0.94583333 0.95416667 0.94166667 0.84583333]\n [0.85833333 0.87083333 0.9        0.84166667 0.87916667 0.8875\n  0.85833333 0.8875     0.82083333 0.88333333 0.875      0.825\n  0.88333333 0.86666667 0.875      0.8625     0.76666667]\n [0.94583333 0.95833333 0.9875     0.92916667 0.96666667 0.975\n  0.94583333 0.975      0.90833333 0.97083333 0.9625     0.9125\n  0.97083333 0.95416667 0.9625     0.95       0.85416667]\n [0.90416667 0.91666667 0.94583333 0.8875     0.925      0.93333333\n  0.90416667 0.93333333 0.86666667 0.92916667 0.92083333 0.87083333\n  0.92916667 0.9125     0.92083333 0.90833333 0.8125    ]\n [0.93333333 0.94583333 0.975      0.91666667 0.95416667 0.9625\n  0.93333333 0.9625     0.89583333 0.95833333 0.95       0.9\n  0.95833333 0.94166667 0.95       0.9375     0.84166667]\n [0.95416667 0.96666667 0.99583333 0.9375     0.975      0.98333333\n  0.95416667 0.98333333 0.91666667 0.97916667 0.97083333 0.92083333\n  0.97916667 0.9625     0.97083333 0.95833333 0.8625    ]\n [0.8375     0.85       0.87916667 0.82083333 0.85833333 0.86666667\n  0.8375     0.86666667 0.8        0.8625     0.85416667 0.80416667\n  0.8625     0.84583333 0.85416667 0.84166667 0.74583333]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a810da36730504177f0741e111e290aa22fe0e7d"
      },
      "cell_type": "code",
      "source": "## This exercise of work is for demonstrating pre-processing techniques only, The model can give around 50% accuracy for now.\n## We got to apply some more data to make it improve it's accuracy as well hyper tuning of parameters in the algorithm.\n\n## The overall problem that the solution covers is to identify type 1 of the pokemon using other features in the dataset.",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}