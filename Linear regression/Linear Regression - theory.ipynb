{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression \n",
    "\n",
    "## Where to use regression\n",
    "1. Predict how to allocate funds optimally and increase sales\n",
    "2. Predict demand for a product\n",
    "3. Help understand drug effects\n",
    "4. Help understand economics\n",
    "\n",
    "##### Regression are to quantify the strength of relationship between one variable and the other variable that are thought to explain it.\n",
    "\n",
    "\n",
    "#### Linear regression - \n",
    "fits a line by data points mapping numerical input to numerical output\n",
    "What you just did is infer your model (that head of yours) to generalize — predict the y-value for an x-value that is not even in your knowledge.\n",
    "\n",
    "n terms of Machine Learning, this follows the convention:\n",
    "h(X) = W0 + W1.X\n",
    "Where W0 and W1 are weights, X is the input feature, and h(X) is the label (i.e. y-value).\n",
    "\n",
    "###### The way Linear Regression works is by trying to find the weights (namely, W0 and W1) that lead to the best-fitting line for the input data (i.e. X features) we have. The best-fitting line is determined in terms of lowest cost.\n",
    "\n",
    "\n",
    "Cost could take different forms, depending on the Machine Learning application at hand. However, in general, cost refers to the loss or error that the model yields in terms of how off it is from the actual Training data.\n",
    "When it comes to Linear Regression, the cost function we usually use is the Squared Error Cost.\n",
    "J(W0,W1) = (1/2n).sigma((h(Xi)-Ti)^2) for all i=1 until i=n\n",
    "Where J(W0,W1) refers to the total cost of the model with weights W0, W1. h(Xi) refers to the model’s prediction of the y-value at feature X with index i. Ti is the actual y-value at index i. And finally, n is the total number of data points in the data set.\n",
    "\n",
    "All what our cost function is doing is basically getting the distance (e.g. Euclidean distance) between what y-value the model predicted and what the actual y-value resident in the data set is for every data point, then squaring this distance and dividing it by the number of data points we have to get the average cost. \n",
    "\n",
    "\n",
    "#### Training a Machine Learning model is all about using a Learning Algorithm to find the weights (W0, W1 in our formula) that minimize the cost. \n",
    "\n",
    "#### Training is basically finding those weights and plugging them into the straight line function so that we have best-fit line (with W0, W1 minimizing the cost). The algorithm basically follows the pseudo-code:\n",
    "\n",
    "Repeat until convergence {\n",
    "    temp0 := W0 - a.((d/dW0) J(W0,W1))\n",
    "    temp1 := W1 - a.((d/dW1) J(W0,W1))\n",
    "    W0 = temp0\n",
    "    W1 = temp1\n",
    "}\n",
    "\n",
    "Where (d/dW0) and (d/dW1) are the partial derivatives of J(W0,W1) with respect to W0 and W1, respectively. The gist of this partial differentiation is basically the derivatives:\n",
    "(d/dW0) J(W0,W1) = W0 + W1.X - T\n",
    "(d/dW1) j(W0,W1) = (W0 + W1.X - T).X\n",
    "\n",
    "\n",
    "###### It turns out that, in terms of Linear Regression, “linear” does not refer to “straight line”, but rather to “falling on one line”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
