{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd  \nimport numpy as np  \nimport matplotlib.pyplot as plt  \nimport seaborn as seabornInstance \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/advertising.csv')\nprint(dataset.shape)\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"## 2D-plot for Money spent on TV Ads and its effect on Sales\ndataset.plot(x='TV', y='Sales', style='o')  \nplt.title('TV vs Sales')  \nplt.xlabel('Money spent on TV Ads')  \nplt.ylabel('Sales')  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 2D-plot for Money spent on Newspaper Ads and its effect on Sales\ndataset.plot(x='Newspaper', y='Sales', style='o')  \nplt.title('Newspaper vs Sales')  \nplt.xlabel('Money spent on Newspaper Ads')  \nplt.ylabel('Sales')  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 2D-plot for Money spent on Radio Ads and its effect on Sales\ndataset.plot(x='Radio', y='Sales', style='o')  \nplt.title('Radio vs Sales')  \nplt.xlabel('Money spent on Radio Ads')  \nplt.ylabel('Sales')  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.tight_layout()\nseabornInstance.distplot(dataset['Sales'])\n## The avg seems to lie nearly between 15 to 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Basic preprocessing to find whether dataset has noisy data\nprint(dataset.isna().any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Our next step is to divide the data into “attributes” and “labels”. \n## Attributes are the independent variables while labels are dependent variables whose values are to be predicted. \nX = dataset['TV'].values.reshape(-1,1) # Attributes/Features\ny = dataset['Sales'].values.reshape(-1,1) # Label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## To split the data into test set(20%) and train set(80%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple linear regression model\nregressor = LinearRegression()  \nregressor.fit(X_train, y_train) #training the algorithm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The linear regression model basically finds the best value for the intercept and slope, which results in a line that best fits the data. \n## To see the value of the intercept and slop calculated by the linear regression algorithm for our dataset\n\n#To retrieve the intercept:\nprint(regressor.intercept_)\n#For retrieving the slope:\nprint(regressor.coef_)\n\n## This means that for every one unit of effect in TV ads, the Effect in the sales is about to increase by 0.054%.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## To predict the test data with our trained model\ny_pred = regressor.predict(X_test)\ndf = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Bar graph representation on actual vs predicted by the model\ndf1 = df\ndf1.plot(kind='bar',figsize=(16,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Below Visualization to see the linear fit of our model with actual and prediction.\n\nplt.scatter(X_test, y_test,  color='gray')\nplt.plot(X_test, y_pred, color='red', linewidth=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## For Regression, there are three metrics to consider which are shown below\n\n## Mean Absolute Error (MAE) is the mean of the absolute value of the errors.\n## Mean Squared Error (MSE) is the mean of the squared errors\n## Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## In the above cells , we handled the problem with involving sales and TV data, where as in the real world problems we need to include certain other variables\n## for consideration. Linear regression involving multiple variables is called “multiple linear regression” or multivariate linear regression.\n\n## The steps to perform multiple linear regression are almost similar to that of simple linear regression. The difference lies in the evaluation. \n## You can use it to find out which factor has the highest impact on the predicted output and how different variables relate to each other.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## In multiple linear regression, till the preprocessing steps above has to be followed and need to proceed further as shown below.\n\n## Dataset \n\nX = dataset[['TV','Newspaper','Radio']]\ny = dataset['Sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## To split the data with multiple independent variables and dependent variable(multi attributes) - Train 80% and Test 20% Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Multiple linear model\nregressor = LinearRegression()  \nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## In the case of multivariable linear regression, the regression model has to find the most optimal coefficients for all the attributes.\n\ncoeff_df = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficient'])  \ncoeff_df\n\n## From the below result which means the following interpretaion\n\n## 1. For 1 unit of TV Ads there is 0.05% increase in Sales\n## 2. For 1 unit of Newspaper Ads there is 0.003% of decrease in Sales\n## 3. For 1 unit of Radio Ads there is 0.11% of increase in Sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting the results with test data after the model has been trained with multiattributes\ny_pred = regressor.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf1 = df.head(25)\nprint(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.plot(kind='bar',figsize=(10,8))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n\n\n## The RMSE value seems to be lesser than the simple linear regression model which is around 2.5, so this makes more efficient prediction with multidimension \n## than the simple model.\n\n\n## Alternate for finding RMSE \nfrom sklearn.model_selection import cross_val_score\nMSEs = cross_val_score(regressor, X, y, scoring='neg_mean_squared_error', cv=5)\nmean_MSE = np.mean(MSEs)\n\nprint(\"Alternative way of representing error :\",mean_MSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On overall, the RMSE value seems to be greater than 10% of the mean of Sales value overall which means the model is not so accuate but we can able to make better predictions though, below are some of the reasons to improve the model accuracy .\n\n1. Need more data: We need to have a huge amount of data to get the best possible prediction. \n2. Poor features: The features we used may not have had a high enough correlation to the values we were trying to predict."},{"metadata":{},"cell_type":"markdown","source":"# ADD-ON "},{"metadata":{},"cell_type":"markdown","source":"### Regularization penalty for linear models\n\nRegularization penalizes parameters for being too large and keeps them from being weighted too heavily. Typically, the penalty grows exponentially, so the larger a coefficient gets, the heavier the penalty.\nRegularization is used to keep your sample's idiosyncrasies from having too much influence on your model. It's a trade-off: you get a more generalized model, but it loses accuracy (in the sample/training set)."},{"metadata":{},"cell_type":"markdown","source":"#### Problem with Linear Regression Model\n\nlinear models actually look at average squared error.\n\nFinding such a line of best fit based on minimizing this mean squared error is actually a really easy problem to solve and there are tons of algorithms (the most common one being ordinary least squares(OLS)) that will find a solution for us. And the solution is guaranteed to be optimal from the perspective of minimizing average squared error within the dataset given.\n\nOne key observation is that even though the linear model may be optimal for the data given to create the model, it is not necessarily guaranteed to be the best model for predictions on unseen data.\nOne of the primary reasons that our model ends up performing on unseen data so poorly is directly related to the complexity of the model. \nIf our underlying data follows a relatively simple model, and the model we use is too complex for the task, what we are essentially doing is we are putting too much weight on any possible change or variance in the data. Our model is overreacting and overcompensating for even the slightest change in our data. People in the field of statistics and machine learning call this phenomenon overfitting.\n"},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression\n\nTo overcome the problem of linear model by adding a penalty to models that have too large coefficients. Remember that fitting a linear model is just a matter of minimizing the average squared error between the true data points and the data points estimated by our model. To get ridge regression, all we do is now we add a constraint that penalizes models with large coefficients.we are actually going to penalize the sum of the squares of the coefficients. \n\nLet's see the Math behind this\n\nThe norm of a vector 𝑤 is denoted as ‖𝑤‖.\n\nWith this knowledge in hand, let’s see how the math works out. In ordinary least squares given a dataset 𝑋 and the true values 𝑦 (in our example the matrix 𝑋 had two columns and three rows), we try to find a set of weights or coefficients 𝑤 that minimizes the average square error or\n\n1𝑁‖𝑋𝑤−𝑦‖2\n\nNow, in ridge regression we try find a set of coefficients 𝑤 to minimize\n\n1𝑁(‖𝑋𝑤−𝑦‖2+𝜆‖𝑤‖2)\n\nNotice that the only difference is the 𝜆‖𝑤‖2. What does this mean? Turns out this is a natural way to express the idea of making sure that our weights do not get too large. Notice that if our coefficients become too large, the norm of 𝑤 becomes increasingly large; consequently, ridge regression will avoid these solutions. And the 𝜆 parameter is something called a hyperparameter that we use to represent the degree in which we want to penalize complex models.\nIn a way, you can think of ridge regression as a way to incorporate prior knowledge into a linear model. If you know ahead of time that you model is likely to be simple, ridge regression is likely more preferable than OLS\n\nThe idea of penalizing complex models through large coefficients is central to ridge regression and a general powerful tool in machine learning and statistics called regularization. Regularization is something which battles against overfitting.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Ridge regression for handling our advertising dataset\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\n\nalpha = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n\nridge = Ridge()\nparameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]} ## 𝜆‖𝑤‖2 - alpha value \nridge_regressor = GridSearchCV(ridge, parameters,scoring='neg_mean_squared_error', cv=5)\nridge_regressor.fit(X, y)\n\nprint(ridge_regressor.best_params_)\nprint(ridge_regressor.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LASSO Regression\n\nThe main goal of introducing Regularization in a statistical model is to avoid over-fitting. The same thing can be said and interpreted in many different ways -  to improve out-of-sample model performance or to reduce model complexity. Another important issue tackled by using Regularization is handling Multicollinearity i.e. it can fit a model to data even when we have correlated features.\n\nThe different method of regularizaiton in the statistical model is LASSO (Least Absolute Shrinkage and Selection Operator)\n\nLasso regression analysis is a shrinkage and variable selection method for linear regression models. The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. The lasso does this by imposing a constraint on the model parameters that causes regression coefficients for some variables to shrink toward zero. Variables with a regression coefficient equal to zero after the shrinkage process are excluded from the model. Variables with non-zero regression coefficients variables are most strongly associated with the response variable.\n\nTo test a lasso regression model, you will need to identify a quantitative response variable from your data set if you haven’t already done so, and choose a few additional quantitative and categorical predictor (i.e. explanatory) variables to develop a larger pool of predictors. Having a larger pool of predictors to test will maximize your experience with lasso regression analysis.\n\nThe lasso regression analysis will help you determine which of your predictors are most important. \n##### Note also that if you are working with a relatively small data set, you do not need to split your data into training and test data sets. The cross-validation method you apply is designed to eliminate the need to split your data when you have a limited number of observations.\n\nFor more detailed math related to regularization refer the below link.\nhttp://www.holehouse.org/mlclass/07_Regularization.html\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## LASSO regression for handling our advertising dataset\n\nfrom sklearn.linear_model import Lasso\n\nlasso = Lasso()\nparameters = {'alpha': [1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\nlasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv = 5)\nlasso_regressor.fit(X, y)\nprint(lasso_regressor.best_params_)\nprint(lasso_regressor.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Thus the regularization methods are applied for the linear regression model by tuning the hyperparamters thus overcoming the problem of multicollinearity and overfitting even for the unseen data prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}